{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Iterable\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import Field, TextField, SequenceLabelField, MetadataField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.dataset_readers.dataset_utils import Ontonotes, OntonotesSentence, to_bioul\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "def _normalize_word(word: str):\n",
    "    if word == \"/.\" or word == \"/?\":\n",
    "        return word[1:]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "class OntonotesNamedEntityRecognition(DatasetReader):\n",
    "    \"\"\"\n",
    "    This DatasetReader is designed to read in the English OntoNotes v5.0 data\n",
    "    for fine-grained named entity recognition. It returns a dataset of instances with the\n",
    "    following fields:\n",
    "\n",
    "    tokens : ``TextField``\n",
    "        The tokens in the sentence.\n",
    "    tags : ``SequenceLabelField``\n",
    "        A sequence of BIO tags for the NER classes.\n",
    "\n",
    "    Note that the \"/pt/\" directory of the Onotonotes dataset representing annotations\n",
    "    on the new and old testaments of the Bible are excluded, because they do not contain\n",
    "    NER annotations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_indexers : ``Dict[str, TokenIndexer]``, optional\n",
    "        We similarly use this for both the premise and the hypothesis.  See :class:`TokenIndexer`.\n",
    "        Default is ``{\"tokens\": SingleIdTokenIndexer()}``.\n",
    "    domain_identifier: ``str``, (default = None)\n",
    "        A string denoting a sub-domain of the Ontonotes 5.0 dataset to use. If present, only\n",
    "        conll files under paths containing this domain identifier will be processed.\n",
    "    coding_scheme : ``str``, (default = None).\n",
    "        The coding scheme to use for the NER labels. Valid options are \"BIO\" or \"BIOUL\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A ``Dataset`` of ``Instances`` for Fine-Grained NER.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 domain_identifier: str = None,\n",
    "                 coding_scheme: str = \"BIO\",\n",
    "                 lazy: bool = False) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self._domain_identifier = domain_identifier\n",
    "        if domain_identifier == \"pt\":\n",
    "            raise ConfigurationError(\"The Ontonotes 5.0 dataset does not contain annotations for\"\n",
    "                                     \" the old and new testament sections.\")\n",
    "        self._coding_scheme = coding_scheme\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str):\n",
    "        # if `file_path` is a URL, redirect to the cache\n",
    "        file_path = cached_path(file_path)\n",
    "        ontonotes_reader = Ontonotes()\n",
    "        logger.info(\"Reading Fine-Grained NER instances from dataset files at: %s\", file_path)\n",
    "        if self._domain_identifier is not None:\n",
    "            logger.info(\"Filtering to only include file paths containing the %s domain\", self._domain_identifier)\n",
    "\n",
    "        for sentence in self._ontonotes_subset(ontonotes_reader, file_path, self._domain_identifier):\n",
    "            tokens = [Token(_normalize_word(t)) for t in sentence.words]\n",
    "            yield self.text_to_instance(tokens, sentence.named_entities)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ontonotes_subset(ontonotes_reader: Ontonotes,\n",
    "                          file_path: str,\n",
    "                          domain_identifier: str) -> Iterable[OntonotesSentence]:\n",
    "        \"\"\"\n",
    "        Iterates over the Ontonotes 5.0 dataset using an optional domain identifier.\n",
    "        If the domain identifier is present, only examples which contain the domain\n",
    "        identifier in the file path are yielded.\n",
    "        \"\"\"\n",
    "        for conll_file in ontonotes_reader.dataset_path_iterator(file_path):\n",
    "            if (domain_identifier is None or f\"/{domain_identifier}/\" in conll_file) and \"/pt/\" not in conll_file:\n",
    "                yield from ontonotes_reader.sentence_iterator(conll_file)\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, # type: ignore\n",
    "                         tokens: List[Token],\n",
    "                         ner_tags: List[str] = None) -> Instance:\n",
    "        \"\"\"\n",
    "        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n",
    "        \"\"\"\n",
    "        # pylint: disable=arguments-differ\n",
    "        sequence = TextField(tokens, self._token_indexers)\n",
    "        instance_fields: Dict[str, Field] = {'tokens': sequence}\n",
    "        instance_fields[\"metadata\"] = MetadataField({\"words\": [x.text for x in tokens]})\n",
    "        # Add \"tag label\" to instance\n",
    "        if ner_tags is not None:\n",
    "            if self._coding_scheme == \"BIOUL\":\n",
    "                ner_tags = to_bioul(ner_tags, encoding=\"BIO\")\n",
    "            instance_fields['tags'] = SequenceLabelField(ner_tags, sequence)\n",
    "        return Instance(instance_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_data_into_pkl(domain, quantity=1000, method = 'iid'):\n",
    "    reader = OntonotesNamedEntityRecognition(domain_identifier=domain,coding_scheme=\"BIOUL\")\n",
    "    train_data_gen = reader._read('datasets/conll-formatted-ontonotes-5.0/data/train/')\n",
    "    train_data = list(train_data_gen)\n",
    "    random.seed(quantity)\n",
    "    random.shuffle(train_data)\n",
    "    ktrain_data = train_data[:quantity]\n",
    "    return ktrain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='bn', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/bn_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='bn', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/bn_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='bn', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/bn_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='nw', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/nw_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='nw', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/nw_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='nw', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/nw_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='bc', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/bc_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='bc', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/bc_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='bc', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/bc_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='tc', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/tc_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='tc', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/tc_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='tc', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/tc_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='pt', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/pt_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='pt', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/pt_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='pt', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/pt_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='mz', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/mz_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='mz', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/mz_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='mz', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/mz_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='wb', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/wb_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='wb', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/wb_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='wb', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets-ner/wb_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
