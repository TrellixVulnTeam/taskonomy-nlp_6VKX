{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Iterable\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import Field, TextField, SequenceLabelField, MetadataField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.dataset_readers.dataset_utils import Ontonotes, OntonotesSentence\n",
    "\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "class SrlReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    This DatasetReader is designed to read in the English OntoNotes v5.0 data\n",
    "    for semantic role labelling. It returns a dataset of instances with the\n",
    "    following fields:\n",
    "\n",
    "    tokens : ``TextField``\n",
    "        The tokens in the sentence.\n",
    "    verb_indicator : ``SequenceLabelField``\n",
    "        A sequence of binary indicators for whether the word is the verb for this frame.\n",
    "    tags : ``SequenceLabelField``\n",
    "        A sequence of Propbank tags for the given verb in a BIO format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_indexers : ``Dict[str, TokenIndexer]``, optional\n",
    "        We similarly use this for both the premise and the hypothesis.  See :class:`TokenIndexer`.\n",
    "        Default is ``{\"tokens\": SingleIdTokenIndexer()}``.\n",
    "    domain_identifier: ``str``, (default = None)\n",
    "        A string denoting a sub-domain of the Ontonotes 5.0 dataset to use. If present, only\n",
    "        conll files under paths containing this domain identifier will be processed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A ``Dataset`` of ``Instances`` for Semantic Role Labelling.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 domain_identifier: str = None,\n",
    "                 lazy: bool = False) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._token_indexers = token_indexers or {\"tokens\":SingleIdTokenIndexer()}\n",
    "        self._domain_identifier = domain_identifier\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str):\n",
    "        # if `file_path` is a URL, redirect to the cache\n",
    "        file_path = cached_path(file_path)\n",
    "        ontonotes_reader = Ontonotes()\n",
    "        logger.info(\"Reading SRL instances from dataset files at: %s\", file_path)\n",
    "        if self._domain_identifier is not None:\n",
    "            logger.info(\"Filtering to only include file paths containing the %s domain\", self._domain_identifier)\n",
    "\n",
    "        for sentence in self._ontonotes_subset(ontonotes_reader, file_path, self._domain_identifier):\n",
    "            tokens = [Token(t) for t in sentence.words]\n",
    "            if not sentence.srl_frames:\n",
    "                # Sentence contains no predicates.\n",
    "                tags = [\"O\" for _ in tokens]\n",
    "                verb_label = [0 for _ in tokens]\n",
    "                yield self.text_to_instance(tokens, verb_label, tags)\n",
    "            else:\n",
    "                for (_, tags) in sentence.srl_frames:\n",
    "                    verb_indicator = [1 if label[-2:] == \"-V\" else 0 for label in tags]\n",
    "                    yield self.text_to_instance(tokens, verb_indicator, tags)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ontonotes_subset(ontonotes_reader: Ontonotes,\n",
    "                          file_path: str,\n",
    "                          domain_identifier: str) -> Iterable[OntonotesSentence]:\n",
    "        \"\"\"\n",
    "        Iterates over the Ontonotes 5.0 dataset using an optional domain identifier.\n",
    "        If the domain identifier is present, only examples which contain the domain\n",
    "        identifier in the file path are yielded.\n",
    "        \"\"\"\n",
    "        for conll_file in ontonotes_reader.dataset_path_iterator(file_path):\n",
    "            if domain_identifier is None or f\"/{domain_identifier}/\" in conll_file:\n",
    "                yield from ontonotes_reader.sentence_iterator(conll_file)\n",
    "\n",
    "    def text_to_instance(self,  # type: ignore\n",
    "                         tokens: List[Token],\n",
    "                         verb_label: List[int],\n",
    "                         tags: List[str] = None) -> Instance:\n",
    "        \"\"\"\n",
    "        We take `pre-tokenized` input here, along with a verb label.  The verb label should be a\n",
    "        one-hot binary vector, the same length as the tokens, indicating the position of the verb\n",
    "        to find arguments for.\n",
    "        \"\"\"\n",
    "        # pylint: disable=arguments-differ\n",
    "        fields: Dict[str, Field] = {}\n",
    "        text_field = TextField(tokens, token_indexers=self._token_indexers)\n",
    "        fields['tokens'] = text_field\n",
    "        fields['verb_indicator'] = SequenceLabelField(verb_label, text_field)\n",
    "        if tags:\n",
    "            fields['tags'] = SequenceLabelField(tags, text_field)\n",
    "\n",
    "        if all([x == 0 for x in verb_label]):\n",
    "            verb = None\n",
    "        else:\n",
    "            verb = tokens[verb_label.index(1)].text\n",
    "        fields[\"metadata\"] = MetadataField({\"words\": [x.text for x in tokens],\n",
    "                                            \"verb\": verb})\n",
    "        return Instance(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_data_into_pkl(domain='bn', quantity=1000, method = 'iid'):\n",
    "    reader = SrlReader(token_indexers= None,\n",
    "                       domain_identifier=domain)\n",
    "    train_data_gen = reader._read('datasets/conll-formatted-ontonotes-5.0/data/train/')\n",
    "    train_data = list(train_data_gen)\n",
    "    random.seed(quantity)\n",
    "    random.shuffle(train_data)\n",
    "    ktrain_data = train_data[:quantity]\n",
    "    return ktrain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='bn', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/bn_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='bn', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/bn_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='bn', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/bn_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='nw', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/nw_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='nw', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/nw_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='nw', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/nw_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='bc', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/bc_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='bc', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/bc_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='bc', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/bc_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='tc', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/tc_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='tc', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/tc_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='tc', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/tc_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='pt', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/pt_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='pt', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/pt_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='pt', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/pt_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='mz', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/mz_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='mz', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/mz_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='mz', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/mz_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pack_data_into_pkl(domain='wb', quantity=1000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/wb_1k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='wb', quantity=5000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/wb_5k.pkl','wb'))\n",
    "\n",
    "train_data = pack_data_into_pkl(domain='wb', quantity=10000, method = 'iid')\n",
    "pkl.dump(train_data, open('datasets/cross-domain-subsets/wb_10k.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
